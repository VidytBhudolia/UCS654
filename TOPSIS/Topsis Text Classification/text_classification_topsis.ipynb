{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e385509",
   "metadata": {},
   "source": [
    "# Text Classification TOPSIS Analysis\n",
    "This notebook performs sentiment analysis on the IMDB dataset using 6 different Hugging Face models and ranks them using TOPSIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf4e3ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from topsis_package import topsis\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"Using device: {device_type}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c15b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 test samples\n",
      "Positive samples: 100, Negative samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"].select(range(1000))\n",
    "\n",
    "# Create a balanced test set with both positive and negative samples\n",
    "test_negative = dataset[\"test\"].filter(lambda x: x[\"label\"] == 0).select(range(100))\n",
    "test_positive = dataset[\"test\"].filter(lambda x: x[\"label\"] == 1).select(range(100))\n",
    "\n",
    "# Combine them\n",
    "from datasets import concatenate_datasets\n",
    "test_data = concatenate_datasets([test_negative, test_positive])\n",
    "\n",
    "# Extract test labels and texts\n",
    "test_texts = test_data[\"text\"]\n",
    "test_labels = test_data[\"label\"]\n",
    "\n",
    "# Convert to lists\n",
    "test_texts = list(test_texts)\n",
    "test_labels = list(test_labels)\n",
    "\n",
    "print(f\"Loaded {len(test_texts)} test samples\")\n",
    "print(f\"Positive samples: {sum(test_labels)}, Negative samples: {len(test_labels) - sum(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34229292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to test: 5\n",
      "1. distilbert-base-uncased-finetuned-sst-2-english\n",
      "2. textattack/bert-base-uncased-SST-2\n",
      "3. siebert/sentiment-roberta-large-english\n",
      "4. cardiffnlp/twitter-roberta-base-sentiment\n",
      "5. nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"textattack/bert-base-uncased-SST-2\",\n",
    "    \"siebert/sentiment-roberta-large-english\",\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "]\n",
    "\n",
    "print(f\"Models to test: {len(models)}\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e31965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30c46734bee4da4902052dda110a74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.8750, F1: 0.8731, Precision: 0.8866, Recall: 0.8600\n",
      "  Pred: 97 pos / 103 neg, Actual: 100 pos / 100 neg\n",
      "  Time: 30.88s, Size: 255.41MB\n",
      "\n",
      "Testing model: textattack/bert-base-uncased-SST-2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4bac61b53e4eaa8be8bffe79b33b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.8800, F1: 0.8846, Precision: 0.8519, Recall: 0.9200\n",
      "  Pred: 108 pos / 92 neg, Actual: 100 pos / 100 neg\n",
      "  Time: 61.62s, Size: 417.65MB\n",
      "\n",
      "Testing model: siebert/sentiment-roberta-large-english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee827a53b2e4594a72ca16733f455b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: siebert/sentiment-roberta-large-english\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9650, F1: 0.9655, Precision: 0.9515, Recall: 0.9800\n",
      "  Pred: 103 pos / 97 neg, Actual: 100 pos / 100 neg\n",
      "  Time: 209.37s, Size: 1355.60MB\n",
      "\n",
      "Testing model: cardiffnlp/twitter-roberta-base-sentiment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698ef0230ac14947a7c79dc45106ee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: cardiffnlp/twitter-roberta-base-sentiment\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.8700, F1: 0.8774, Precision: 0.8304, Recall: 0.9300\n",
      "  Pred: 112 pos / 88 neg, Actual: 100 pos / 100 neg\n",
      "  Time: 61.02s, Size: 475.49MB\n",
      "\n",
      "Testing model: nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7472b2be8a41ceb0345ed9fdbd7545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.7850, F1: 0.8186, Precision: 0.7080, Recall: 0.9700\n",
      "  Pred: 137 pos / 63 neg, Actual: 100 pos / 100 neg\n",
      "  Time: 65.46s, Size: 638.43MB\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load pipeline with timeout settings\n",
    "        classifier = pipeline(\"sentiment-analysis\", model=model_name, device=device, revision=\"main\")\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        predictions = classifier(test_texts, truncation=True, max_length=512)\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Process predictions\n",
    "        pred_labels = []\n",
    "        for pred in predictions:\n",
    "            label = pred[\"label\"]\n",
    "            score = pred.get(\"score\", 0.5)\n",
    "            # Map different label formats to binary (0=negative, 1=positive)\n",
    "            if label in [\"POSITIVE\", \"POS\", \"LABEL_1\"]:\n",
    "                pred_labels.append(1)\n",
    "            elif label in [\"NEGATIVE\", \"NEG\", \"LABEL_0\"]:\n",
    "                pred_labels.append(0)\n",
    "            elif label in [\"neutral\", \"LABEL_2\"]:\n",
    "                # For neutral, map based on score\n",
    "                pred_labels.append(1 if score > 0.5 else 0)\n",
    "            else:\n",
    "                # For multilingual models that output star ratings or other formats\n",
    "                if \"star\" in label.lower():\n",
    "                    stars = int(label.split()[0])\n",
    "                    pred_labels.append(1 if stars >= 3 else 0)\n",
    "                else:\n",
    "                    # Check score for generic binary classification\n",
    "                    pred_labels.append(1 if score > 0.5 else 0)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = sum([1 for i in range(len(test_labels)) if pred_labels[i] == test_labels[i]]) / len(test_labels)\n",
    "        \n",
    "        # Count predictions\n",
    "        num_positive_pred = sum(pred_labels)\n",
    "        num_negative_pred = len(pred_labels) - num_positive_pred\n",
    "        num_positive_actual = sum(test_labels)\n",
    "        num_negative_actual = len(test_labels) - num_positive_actual\n",
    "        \n",
    "        # Calculate F1 score \n",
    "        tp = sum(1 for i in range(len(test_labels)) if test_labels[i] == 1 and pred_labels[i] == 1)\n",
    "        fp = sum(1 for i in range(len(test_labels)) if test_labels[i] == 0 and pred_labels[i] == 1)\n",
    "        fn = sum(1 for i in range(len(test_labels)) if test_labels[i] == 1 and pred_labels[i] == 0)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Get model size\n",
    "        model_path = classifier.model.config._name_or_path\n",
    "        model_size = 0\n",
    "        try:\n",
    "            # Estimate model size from parameters\n",
    "            total_params = sum(p.numel() for p in classifier.model.parameters())\n",
    "            # Approximate size in MB (assuming float32 = 4 bytes per parameter)\n",
    "            model_size = (total_params * 4) / (1024 * 1024)\n",
    "        except:\n",
    "            model_size = 0\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1\": f1_score,\n",
    "            \"Inference_Time\": inference_time,\n",
    "            \"Model_Size_MB\": model_size\n",
    "        })\n",
    "        \n",
    "        print(f\"  Accuracy: {accuracy:.4f}, F1: {f1_score:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "        print(f\"  Pred: {num_positive_pred} pos / {num_negative_pred} neg, Actual: {num_positive_actual} pos / {num_negative_actual} neg\")\n",
    "        print(f\"  Time: {inference_time:.2f}s, Size: {model_size:.2f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading model: {str(e)[:100]}...skipping this model\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b11e480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Results:\n",
      "                                           Model  Accuracy       F1  Inference_Time  Model_Size_MB\n",
      " distilbert-base-uncased-finetuned-sst-2-english     0.875 0.873096       30.882878     255.413094\n",
      "              textattack/bert-base-uncased-SST-2     0.880 0.884615       61.624366     417.647469\n",
      "         siebert/sentiment-roberta-large-english     0.965 0.965517      209.374728    1355.597664\n",
      "       cardiffnlp/twitter-roberta-base-sentiment     0.870 0.877358       61.022336     475.494152\n",
      "nlptown/bert-base-multilingual-uncased-sentiment     0.785 0.818565       65.462829     638.428730\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nInitial Results:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8633d0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying TOPSIS with:\n",
      "  Weights: [0.3, 0.3, 0.2, 0.2]\n",
      "  Criteria: Accuracy (+), F1 (+), Inference_Time (-), Model_Size_MB (-)\n",
      "  Impacts: ['+', '+', '-', '-']\n",
      "\n",
      "TOPSIS applied successfully\n"
     ]
    }
   ],
   "source": [
    "# Apply TOPSIS\n",
    "weights = [0.3, 0.3, 0.2, 0.2]\n",
    "impacts = ['+', '+', '-', '-']\n",
    "\n",
    "print(f\"\\nApplying TOPSIS with:\")\n",
    "print(f\"  Weights: {weights}\")\n",
    "print(f\"  Criteria: Accuracy (+), F1 (+), Inference_Time (-), Model_Size_MB (-)\")\n",
    "print(f\"  Impacts: {impacts}\")\n",
    "\n",
    "# Prepare data for TOPSIS (exclude Model column)\n",
    "topsis_data = df[[\"Accuracy\", \"F1\", \"Inference_Time\", \"Model_Size_MB\"]].values\n",
    "\n",
    "# Apply TOPSIS using topsis_package\n",
    "result = topsis(topsis_data, weights, impacts)\n",
    "\n",
    "# Add TOPSIS results to DataFrame\n",
    "df[\"TOPSIS_Score\"] = result.scores\n",
    "df[\"Rank\"] = result.ranks\n",
    "\n",
    "print(\"\\nTOPSIS applied successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6003ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "FINAL RESULTS - MODELS RANKED BY TOPSIS SCORE\n",
      "========================================================================================================================\n",
      "                                           Model  Accuracy       F1  Inference_Time  Model_Size_MB  TOPSIS_Score  Rank\n",
      " distilbert-base-uncased-finetuned-sst-2-english     0.875 0.873096       30.882878     255.413094      0.911201     1\n",
      "              textattack/bert-base-uncased-SST-2     0.880 0.884615       61.624366     417.647469      0.820434     2\n",
      "       cardiffnlp/twitter-roberta-base-sentiment     0.870 0.877358       61.022336     475.494152      0.797925     3\n",
      "nlptown/bert-base-multilingual-uncased-sentiment     0.785 0.818565       65.462829     638.428730      0.695399     4\n",
      "         siebert/sentiment-roberta-large-english     0.965 0.965517      209.374728    1355.597664      0.149865     5\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sort by rank\n",
    "df = df.sort_values(\"Rank\")\n",
    "\n",
    "# Display final DataFrame\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINAL RESULTS - MODELS RANKED BY TOPSIS SCORE\")\n",
    "print(\"=\"*120)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
